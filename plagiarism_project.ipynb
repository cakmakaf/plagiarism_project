{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plagiarism Detection Project\n",
    "\n",
    "In this notebook, you will be tasked with building a plagiarism detector that examines a text file and performs binary classification; labeling that file as either plagiarized or not, depending on how similar the text file *is* to a provided source text. \n",
    "\n",
    "This task will be broken down into a few discrete steps:\n",
    "\n",
    ">1. Load in and explore the corpus.\n",
    "2. Clean and pre-process the data.\n",
    "3. Define metrics for comparing the similarity of a given text file and an original text, and extract similarity features.\n",
    "4. Select \"good\" features, by analyzing the correlations between different features.\n",
    "5. Train a classifier, on the features you selected in step 4, to perform binary classification: plagiarized or not!\n",
    "6. Evaluate your classifier and answer some questions about your approach.\n",
    "\n",
    "You'll be defining a few different similarity metrics, as outlined in [this paper](https://s3.amazonaws.com/video.udacity-data.com/topher/2019/January/5c412841_developing-a-corpus-of-plagiarised-short-answers/developing-a-corpus-of-plagiarised-short-answers.pdf), which should help you build a robust plagiarism detector!\n",
    "\n",
    "To complete this notebook, you'll have to complete all given exercises and answer all the questions in this notebook.\n",
    "> All your tasks will be clearly labeled **EXERCISE** and questions as **QUESTION**.\n",
    "\n",
    "It will be up to you to explore different classification models and decide on a model that gives you the best performance for this dataset. So, first, let's load in and look at the data for this project.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the Libraries and Files\n",
    "\n",
    "The first step in working with any dataset is loading the data in and noting what information is included in the dataset. This is an important step in eventually working with this data, and knowing what kinds of features you have to work with as you transform and group the data!\n",
    "\n",
    "This plagiarism dataset is made of multiple text files; each of these files has characteristics that are is summarized in a `.csv` file named `file_information.csv`, which we can read in using `pandas`.\n",
    "\n",
    "If you are working locally, you'll need to download the necessary text and file data [at this link](https://s3.amazonaws.com/video.udacity-data.com/topher/2019/January/5c4147f9_data/data.zip). It's assumed that this data is extacted and in a folder `data/`.\n",
    "\n",
    "\n",
    "> Each text file is associated with **one task** (task A-E) and **one category of plagiarism**, which you will see described as characters or strings in the following dataframe.\n",
    "\n",
    "As you look at the data, you'll also notice a few other features that were recorded about the person generating the text and the task difficulty level. This dataset was created by Paul Clough (Information Studies) and Mark Stevenson (Computer Science), at the University of Sheffield. You can read all about the data collection and corpus, at [their university webpage](https://ir.shef.ac.uk/cloughie/resources/plagiarism_corpus.html). \n",
    "\n",
    "> **Citation for data**: Clough, P. and Stevenson, M. Developing A Corpus of Plagiarised Short Answers, Language Resources and Evaluation: Special Issue on Plagiarism and Authorship Analysis, In Press. [Download]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_file = 'data/file_information.csv'\n",
    "plagiarism_df = pd.read_csv(csv_file)\n",
    "\n",
    "# print out the first 10 rows of data info\n",
    "plagiarism_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Plagiarism\n",
    "\n",
    "To detect cases of plagiarism, we are most interested in the `Task` and `Category` columns above.\n",
    "\n",
    "###  Five task types, A-E\n",
    "* Each text file contains an answer to one short answer question; these questions are labeled as tasks A-E.\n",
    "* Each task, A-E, is about a topic that might be included in the Computer Science curriculum that was created by the authors of this dataset (cited below). \n",
    "* For each of these questions a set of answers were obtained using a variety of approaches, some of which simulate cases in which the answer *is plagiarized* and others that simulate the case in which the answer *is not plagiarized*.\n",
    "\n",
    "### Four categories of plagiarism \n",
    "\n",
    "To simulate plagiarism, the creators used a Wikipedia entry as a source text to provide to participants. Then the participants were given one of the following levels of plagiarism:\n",
    "\n",
    "1. `cut`: Participants were asked to answer the question by simply copying and pasting text from the relevant Wikipedia article.\n",
    "2. `light`: Participants were asked to paraphrase and base their answer on text found in the Wikipedia article.\n",
    "3. `heavy`: Participants were once again asked to base their answer on the relevant Wikipedia article but were instructed to rephrase the text to generate an answer with the same meaning as the source text, but expressed using different words and structure.\n",
    "4. `non`: Participants were provided with learning materials in the form of either lecture notes or sections from textbooks that could be used to answer the relevant question. Participants were asked to read these materials and then attempt to answer the question using their *own* knowledge and research.\n",
    "5. `orig`: This is a specific category for the original, source text. We will use these files only for comparison purposes.\n",
    "\n",
    "> So, out of the submitted files, the only category that does not contain any plagiarism is `non`.\n",
    "\n",
    "In the next cell, print out some stats about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print out some stats about the data\n",
    "print('Number of files: ', plagiarism_df.shape[0])  # .shape[0] gives the rows \n",
    "# .unique() gives unique items in a specified column\n",
    "print('Number of unique tasks/question types (A-E): ', (len(plagiarism_df['Task'].unique())))\n",
    "print('Unique plagiarism categories: ', (plagiarism_df['Category'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the number of text files in the dataset as well as the name of each file and some associated characteristics. **Note that this *includes* the 5 _original_ wikipedia files for tasks A-E.** If you take a look at the `data_files` directory, you'll notice that the originals start with the filename `orig_` as opposed to `g` for \"group.\" So, in total there are 100 files, 95 of which are answers (submitted by people) and 5 of which are the original source texts.\n",
    "\n",
    "Your goal will be to use this information, and the contents of a given text file, to eventually classify any given text file into one of two categories, plagiarized or not-plagiarized, by comparing it to the original source text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Data\n",
    "\n",
    "Next, let's look at the distribution of data. In this course, we've talked about the importance of datasets in informing how you develop an algorithm. So, here, we'll ask: **how evenly is our data distributed among different tasks and plagiarism levels?**\n",
    "\n",
    "For many machine learning algorithms, we want a large dataset that is fairly evenly distributed among the class we are interested in (in this case, text that is plagiarized or not). If we have a very small dataset or class imbalance, we'll have to be very careful about how we define a model, later on, to get the best results!\n",
    "\n",
    "Below, you should notice two things:\n",
    "* Our dataset is quite small, especially with respect to examples of varying plagiarism levels. This is one of the reasons why we've decided to make this a binary classification project; this way, we can group light, heavy and cut categories into one larger set of plagiarized examples.\n",
    "* The data is distributed fairly evenly across task and plagiarism types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show counts by different tasks and amounts of plagiarism\n",
    "\n",
    "# group and count by task\n",
    "counts_per_task=plagiarism_df.groupby(['Task']).size().reset_index(name=\"Counts\")\n",
    "print(\"\\nTask:\")\n",
    "display(counts_per_task)\n",
    "\n",
    "# group by plagiarism level\n",
    "counts_per_category=plagiarism_df.groupby(['Category']).size().reset_index(name=\"Counts\")\n",
    "print(\"\\nPlagiarism Levels:\")\n",
    "display(counts_per_category)\n",
    "\n",
    "# group by task AND plagiarism level\n",
    "counts_task_and_plagiarism=plagiarism_df.groupby(['Task', 'Category']).size().reset_index(name=\"Counts\")\n",
    "print(\"\\nTask & Plagiarism Level Combos :\")\n",
    "display(counts_task_and_plagiarism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pre-Process the Data\n",
    "\n",
    "Now that you've explored the data a bit, you'll need to pre-process this data. These steps will mainly include converting categorical data (like plagiarism labels) into **numerical data** that we can use as input into a model.\n",
    "\n",
    "In the next few cells, you'll be tasked with creating a new dataframe of desired information about all of the files in the `data_files/` directory. For each file, you'll want to keep track of the text that each file contains, the corresponding task (a-e), and the plagiarism level. Most of this data can be retrieved or calculated from the `.csv` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: Convert categorical to numerical data\n",
    "\n",
    "Complete the below function `clean_dataframe` that reads in a `.csv` file by name, and returns a *new* dataframe with numerical data. The new dataframe should have the following properties:\n",
    "\n",
    "1. 3 columns: `File`, `Task`, `Category`. For the most part these contain the same information as can be found in the original `.csv` file, with a few exceptions.\n",
    "2. Convert all `Category` labels to numerical labels according to the following rules:\n",
    "    * 0 is no plagiarism, coded with Category 'non'.\n",
    "    * 1 is that there was low use of the original file, coded with Category 'heavy' to indicate heavy revision of text from the original answer.\n",
    "    * 2 is that there was medium use of the original file, coded with Category 'light' to indicate only light revision text from the original answer.\n",
    "    * 3 is that there was heavy plagiarism of the original file, coded with Category 'cut' to indicate the answer was cut and pasted 'as-is' from original answer (no revision).\n",
    "    * -1 indicates an original file, coded with Category `orig`.\n",
    "3. Your function should return this cleaned dataframe of 100 rows.\n",
    "\n",
    "### Tips for completing this exercise\n",
    "\n",
    "Your complete code should correctly parse the `.csv` file that contains filenames and plagiarism levels.\n",
    "\n",
    "After running your function, you should get a dataframe with rows that look something like the following: \n",
    "```\n",
    "      File          Task  Category\n",
    "0\tg0pA_taska.txt\ta\t0\n",
    "1\tg0pA_taskb.txt\tb\t3\n",
    "2\tg0pA_taskc.txt\tc\t2\n",
    "3\tg0pA_taskd.txt\td\t1\n",
    "4\tg0pA_taske.txt\te\t0\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_dataframe(csv_file='data/file_information.csv'):\n",
    "    '''This function reads in a dataframe and converts the 'Category' column\n",
    "       from categorical to numerical data.\n",
    "       :param csv_file: the directory for the csv file to be read in\n",
    "       :return: a dataframe with numerical \"Category\" data'''\n",
    "    \n",
    "    # read in csv file\n",
    "    # create a cdataframe that has columns for all the data in the csv file\n",
    "    # and that replaces categorical with numerical data\n",
    "    clean_df = None \n",
    "\n",
    "    return clean_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test cell\n",
    "\n",
    "Below is a test cell. The goal of a cell like this is to ensure that your code is working as expected, and to form any variables that might be used in later tests/code, in this case, the data frame `clean_df`.\n",
    "\n",
    "These tests are not completely rigorous, but they are a great way to check that you are on the right track!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "import problem_unittests as tests\n",
    "\n",
    "# test clean_dataframe function\n",
    "tests.test_clean_df(clean_dataframe)\n",
    "\n",
    "# if passed, create new `clean_df`\n",
    "clean_df = clean_dataframe(csv_file ='data/file_information.csv')\n",
    "\n",
    "# check work\n",
    "clean_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: Pre-process text data & add useful columns to a dataframe\n",
    "\n",
    "Next, you'll add some additional information to your dataframe. Recall that the end goal of this project is to look at the text in one file, compare it to an original file, and label that text as either _plagiarized_ or _not_. To complete this task, you'll need to add some information to your existing dataframe: \n",
    "\n",
    "1. Add a column `Text` that includes the text of each file; this should be lowercase text with any non-alphanumeric characters, and extraneous whitespace characters, removed.\n",
    "2. Add a column `Class` that labels text as `0` not-plagiarized, `1` plagiarized, or `-1` original text; these values should be informed by the existing `Category` column.\n",
    "\n",
    "\n",
    "### Tips for completing this exercise\n",
    "\n",
    "You've been given some text pre-processing code that you can use to help you complete this function. \n",
    "\n",
    "After running your function, you should get a dataframe with rows that look something like the following: \n",
    "```\n",
    "          File\t   Task  Category\t       Text\t                                 Class\n",
    "0\tg0pA_taska.txt\ta\t   0\tinheritance is a basic concept of object orien...\t0\n",
    "1\tg0pA_taskb.txt\tb\t   3\tpagerank is a link analysis algorithm used by ...\t1\n",
    "2\tg0pA_taskc.txt\tc\t   2\tthe vector space model also called term vector...\t1\n",
    "3\tg0pA_taskd.txt\td\t   1\tbayes theorem was names after rev thomas bayes...\t1\n",
    "...\n",
    "95\torig_taska.txt\ta\t  -1\tin object oriented programming inheritance is ...\t-1\n",
    "96\torig_taskb.txt\tb\t  -1\tpagerank is a link analysis algorithm used by ...\t-1\n",
    "97\torig_taskc.txt\tc\t  -1\tvector space model or term vector model is an ...\t-1\n",
    "...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text pre-processing helper function\n",
    "\n",
    "The below helper function need not be changed, it takes in a file and returns an `all_text` lowercase result with some special characters and white spaces removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function for pre-processing text given a file\n",
    "import re\n",
    "\n",
    "def process_file(file):\n",
    "    # put text in all lower case letters \n",
    "    all_text = file.read().lower()\n",
    "\n",
    "    # remove all non-alphanumeric chars\n",
    "    all_text = re.sub(r\"[^a-zA-Z0-9]\", \" \", all_text)\n",
    "    # remove newlines/tabs, etc. so it's easier to match phrases, later\n",
    "    all_text = re.sub(\"  \", \" \", all_text)\n",
    "    all_text = re.sub(r\"\\t\", \" \", all_text)\n",
    "    all_text = re.sub(r\"\\n\", \" \", all_text)\n",
    "    \n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Add new columns `Text` and `Class` to the dataframe\n",
    "\n",
    "This function assumes that you are passing in a `clean_df` as was created above, with a numerical Category column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def complete_dataframe(clean_df, files, file_directory='data/'):\n",
    "    '''Creates a dataframe with additional Text and Class columns.\n",
    "       :param clean_df: a dataframe of file names, tasks, and numerical categories\n",
    "       :param files: a list of files in a directory\n",
    "       :param file_directory: the dir where our text files are stored, default='data/'\n",
    "       :return: df, a dataframe holding the filename, task, category, text, and class\n",
    "    '''\n",
    "    \n",
    "    # create a new dataframe with Text and Class columns\n",
    "    complete_df = None\n",
    "    \n",
    "    return complete_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "# test complete_dataframe function\n",
    "# params: clean_df from before, and complete_dataframe function\n",
    "tests.test_complete_df(clean_df, complete_dataframe)\n",
    "\n",
    "# if passed, create complete `df`\n",
    "# Creates a list of filenames from 'txt' files in data/ directory\n",
    "files = [file for file in os.listdir('data/') if file.endswith('.txt')]\n",
    "\n",
    "# Calls function from above, passing in clean_df from previous step, gets new dataframe\n",
    "df = complete_dataframe(clean_df, files)\n",
    "\n",
    "# check results\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You can also read the full, processed answer text by printing out the `Text` at a certain index in the dataframe. You should see a resultant text that is all lowercase and without punctuation; this will make the texts easier to compare and analyze, later on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take a look at an example answer Text\n",
    "test_index = 0 # first file/row\n",
    "\n",
    "print(df['Text'][test_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "## Split data into training and test for modeling\n",
    "\n",
    "The next cell will adds a column (*Datatype*) to a given dataframe to indicate if the record is: \n",
    "* `orig` - The tasks original answer from wikipedia.\n",
    "* `train` - Training data for model training.\n",
    "* `test` - Testing data for model evaluation.\n",
    "\n",
    "The method below uses a helper function which you can view in the `helpers.py` file in the main project directory. This implements [stratified random sampling](https://en.wikipedia.org/wiki/Stratified_sampling) to randomly split data by task & plagiarism amount. Approximately 26% of the data is held out for testing and 74% of the data is used for training.\n",
    "\n",
    "### Stratified sampling\n",
    "\n",
    "The function **train_test_dataframe** splits the data in a dataframe using stratified sampling on plagiarism amount(0,1-3). It takes in a datframe that it assumes has `Task` and `Category` columns, and, returns a modified frame that indicates which Datatype a file falls into; this sampling will change slightly based on a passed in *random_seed*. Due to small sample size, this stratified random sampling will provides more stable results for a binary plagiarism classifier. Stability here is smaller *variance* in the accuracy of classifier, given a random seed.\n",
    "\n",
    "### Locating a task's original (wiki) answer\n",
    "\n",
    "After adding a `Datatype` column, I'll also add one more column, which will be helpful for comparing a submitted answer to an original source text.\n",
    "\n",
    "A call to `helpers.add_orig_loc(df)` will add a column `Orig_idx` to the passed in dataframe to indicate the index (row) of the task's original answer (from wikipedia). This will be helpful in creating features that rely on comparing a submitted and source text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_seed = 1 # can change; set for reproducibility\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "import helpers\n",
    "\n",
    "# create new df with Datatype (train, test, orig) column\n",
    "df = helpers.train_test_dataframe(df, random_seed=random_seed)\n",
    "# add Orig_idx column; this modifies df directly\n",
    "helpers.add_orig_loc(df)\n",
    "\n",
    "# check results\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Determining Plagiarism\n",
    "\n",
    "Now that you've prepared this data, you can move on to the task of determining whether or not a given file is plagiarized or not. Let's review our goal:\n",
    "> **Goal**: Build a model to classify a given text file as plagiarized or not.\n",
    "\n",
    "This is a binary classification task and now you have all the data you need to build this! The `df` created above has a list of all our data files, the processed text within each file, and the task and class label for that file (0 = not plagiarized, 1 = plagiarized). \n",
    "\n",
    "For example, say you want to train a model to determine if the first file `g0pA_taska.txt` is plagiarized (1) or not (0). You'll first want to look at the text of that file *and* the corresponding original source text file; this is a task A file and so you'll want to compare it to the text of `orig_taska.txt`. You can extract features that measure the similarity of these two texts and feed those into a model that will classify `g0pA_taska.txt` as plagiarized or not.\n",
    "\n",
    "To build this model, the rest of this project is broken down into exercises that will be all about:\n",
    "* Creating relevant, plagiarism-detection features\n",
    "* Defining a model that is trained on the features and the training data/labels\n",
    "* Evaluating the model on our test data\n",
    "\n",
    "It *is* assumed, for ease of testing, that you will not get rid of or rename the original column values and names from above [File, Task, Category, Text, Class, Datatype, Orig_idx]. But, as you go, you're encouraged to develop as many helper functions or *additional* dataframes/columns, as you see fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Similarity Metrics \n",
    "\n",
    "One of the ways we might go about detecting plagiarism, is by computing **similarity metrics** that measure how similar a given text answer is as compared to the original wikipedia answer (for a specific task, a-e). The similarity metrics we will use are informed by the [plagiarism paper](https://s3.amazonaws.com/video.udacity-data.com/topher/2019/January/5c412841_developing-a-corpus-of-plagiarised-short-answers/developing-a-corpus-of-plagiarised-short-answers.pdf) that is also linked at the top of this notebook. In this paper, researchers created features called **containment** and **longest common subsequence**. \n",
    "\n",
    "Using these features as input, we will train a model (naive bayes or logistic) to distinguish between plagiarized and valid text files. Then we will evaluate the accuracy of the model by testing it on our test data.\n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "Let's talk a bit more about what features we want to include in a plagiarism detection model and to calculate such features. In the following explanations, I'll refer to a submitted text file as a **Student Task Answer Text** and the original, wikipedia source file (that we want to compare that answer to) as the **Wikipedia Task Text**.\n",
    "\n",
    "### Containment\n",
    "\n",
    "Your first task will be to create **containment features**.\n",
    "\n",
    "> Containment is defined as the **intersection** of the n-gram word count of the Wikipedia Task Text with the n-gram word count of the Student Task Answer Text *divided* by the n-gram word count of the Student Task Answer Text. In other words, this is a measure, between 0 and 1, of how many n-grams these two texts have in common; you can calculate this value using count or tf-idf vectorization.\n",
    "\n",
    "In this project, it will be up to you to decide on the appropriate `n` or several useful `n`'s to use in your final model.\n",
    "\n",
    "### EXERCISE: Create containment features\n",
    "\n",
    "Given the dataframe that you've created, you should have all the information you need to compare any Student Task Answer Text with its appropriate Wikipedia Task Text (the source text). An answer for task A should be compared to the source text for task A, just as answers to task's B, C, D, and E should be compared to the corresponding original source text.\n",
    "\n",
    "In this exercise, you'll complete the function, `calculate_containment` which calculates containment based upon an passed in dataframe (such as the one you created above with all the text and task information), an input *n-gram* size (a positive integer), which is used to create a specific n-gram containment value, and a file_index, which specifies the row number of the file we are interested in calculating the containment value of.\n",
    "\n",
    "### Tips for completing this exercise\n",
    "\n",
    "The general steps to complete this function are as follows, given a passed in dataframe and n-gram length like 1, 2, 3, etc.\n",
    "1. Calculate n-gram counts for every one of our 100 files. It is suggested that you use a [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) or [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) that can count n-grams when applied to a text.\n",
    "2. For the specified Student Task Answer Text (as specified by `file_index`, compare it's n-gram count to the corresponding Wikipedia Task Text n-gram count to calculate the containment, according to the equation below: the intersection of the source and student n-gram counts *divided* by the student n-gram count.\n",
    "\n",
    "    >$$ \\frac{\\text{n-gram}_{student} \\cap \\text{n-gram}_{original}}{\\text{n-gram}_{student}} $$\n",
    "    \n",
    "3. Return the containment value for that file.\n",
    "\n",
    "To complete this `calculate_containment` function, you may define additional, helper functions in this notebook as you see fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Function that calculates containment for ALL files in df, for specified n-grams\n",
    "# returns containment value for the file in passed in `file_index` row\n",
    "def calculate_containment(df, n, file_index):\n",
    "    ''' Calculates the containment for specified n-grams and for a file in\n",
    "       a passed in `file_index` row.\n",
    "       :param df: complete dataframe with rows [File, Task, Categry, Text, Class]\n",
    "       :param n: n for defining the length of an n-gram count\n",
    "       :param file_index: the index for the file this function should analyze.\n",
    "       :return: containment value for the file located at `file_index`.'''\n",
    "\n",
    "    # create a matrix of ngram counts (iterate over all text files)\n",
    "    # calculate the the intersection of ngram counts between a student answer and original text\n",
    "    # return the normalized containment value\n",
    "    containment_val = None\n",
    "\n",
    "    return containment_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing your implementation\n",
    "\n",
    "After you've implemented this function, you can test out its behavior. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "# test containment calculation\n",
    "# params: complete_df from before, and containment function\n",
    "tests.test_containment(df, calculate_containment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**See the results for yourself!**\n",
    "\n",
    "The below code iterates through the first few files, and calculates the containment values for a specified n.\n",
    "\n",
    ">If you've implemented this correctly, you should see that the non-plagiarized have low or close to 0 containment values and that plagiarized examples have higher containment values (higher for more severe, cut-and-paste-like cases). You may change the value of n and you should generally see higher containment values for smaller values of n.\n",
    "\n",
    "I recommend applying your code to multiple files and comparing the resultant containment values. You should see that the highest containment values correspond to files with the highest category (3) of plagiarism level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# see results for yourself!\n",
    "# do higher containment values correspond to higher plagiarism levels?\n",
    "\n",
    "test_indices = range(5) # first few files\n",
    "n = 3\n",
    "\n",
    "for i in test_indices:\n",
    "    print('Category of plagiarism (3 = highest level, cut): ', df.loc[i, 'Category'])\n",
    "    test_val = calculate_containment(df, n, file_index=i)\n",
    "    print(str(n)+ '-gram containment value: '+ str(test_val))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 1: Does containment calculated using TF-IDF vectorization give same containment value as that calculated using count vectorization? Why or why not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "(Write here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 2: Why can we calculate features across *all* data (training & test), prior to splitting the dataframe for modeling? That is, what about the calculated features makes it so the test and training data do not influence each other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "(Write here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longest Common Subsequence\n",
    "\n",
    "Containment a good way to find overlap in word usage between two documents; it may help identify cases of cut-and-paste as well as paraphrased levels of plagiarism. Since plagiarism is a fairly complex task with varying levels, it's often useful to include other measures of similarity. The paper also discusses a feature called **longest common subsequence**.\n",
    "\n",
    "> The longest common subsequence is the longest string of words (or letters) that are *the same* between the Wikipedia Task Text and the Student Task Answer. This value is also normalized by dividing by the total number of words (or letters) in the  Student Task Answer. \n",
    "\n",
    "In this exercise, we'll ask you to calculate the longest common subsequence of words.\n",
    "\n",
    "### EXERCISE: Calculate the longest common subsequence\n",
    "\n",
    "Complete the function `lcs_norm_word`; this should calculate the *longest common subsequence* of words between a Student Task Answer Text and corresponding Wikipedia Task Text. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Tips to complete this exercise\n",
    "\n",
    "This algorithm depends on looking at two texts and comparing them word by word. You can solve this problem in multiple ways. First, it may be useful to `.split()` each text into lists of comma separated words to compare. Then, you can iterate through each word in the texts and compare them, adding to your value for LCS as you go. \n",
    "\n",
    "The method I recommend for implementing an efficient LCS algorithm is: using a matrix and dynamic programming. **Dynamic programming** is all about breaking a larger problem into a smaller set of subproblems, and building up a complete result without having to repeat any subproblems. \n",
    "\n",
    "This approach assumes that you can split up a large LCS task into a combination of smaller LCS tasks. Let's look at a simple example that compares letters:\n",
    "\n",
    "* S = \"ABCD\"\n",
    "* O = \"BD\"\n",
    "\n",
    "We can see right away that the longest subsequence of _letters_ here is 2 (B and D are in sequence in both strings). And we can calculate this by looking at relationships between each letter in the two strings, S and O. You can also use a matrix to find the LCS.\n",
    "\n",
    "### The matrix rules\n",
    "\n",
    "You can break up an LCS task into a series of smaller tasks and efficiently fill up an LCS matrix one cell at a time. Each grid cell only depends on the values in the grid cells that are directly on top and to the left of it, or on the diagonal/top-left. The rules are as follows:\n",
    "* Start with a matrix that has one extra row and column of zeros.\n",
    "* As you traverse your string:\n",
    "    * If there is a match, fill that grid cell with the value to the top-left of that cell *plus* one. So, in our case, when we found a matching B-B, we added +1 to the value in the top-left of the matching cell, 0.\n",
    "    * If there is not a match, take the *maximum* value from either directly to the left or the top cell, and carry that value over to the non-match cell.\n",
    "\n",
    "<img src='notebook_ims/matrix_rules.png' width=40% />\n",
    "\n",
    "* After completely filling the matrix, **the bottom-right cell will hold the non-normalized LCS value**.\n",
    "\n",
    "This matrix treatment can be applied to a set of words instead of letters. Your function should apply this to the words in two texts and return the normalized LCS value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def lcs_norm_word(source_text, answer_text):\n",
    "    '''Calculates the normalized LCS between a source and answer text.\n",
    "       :param source_text: pre-processed Wikipedia source text\n",
    "       :param answer_text: pre-processed submitted answer text\n",
    "       :return: normalized LCS value between the two texts.'''\n",
    "    \n",
    "    # Split texts into lists of words\n",
    "    # Calculate normalized LCS\n",
    "    lcs_norm = None\n",
    "    \n",
    "    return lcs_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing your implementation \n",
    "\n",
    "Let's start by testing out your code on the example given in the initial description.\n",
    "\n",
    "In the below cell, we have specified strings S (submitted answer) and O (original source text). We know that these texts have 20 words in common and the submitted answer is 27 words long, so the normalized, longest common subsequence should be 20/27.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the test scenario from above\n",
    "# does your function return the expected value?\n",
    "\n",
    "S = \"i think pagerank is a link analysis algorithm used by google that uses a system of weights attached to each element of a hyperlinked set of documents\"\n",
    "O = \"pagerank is a link analysis algorithm used by the google internet search engine that assigns a numerical weighting to each element of a hyperlinked set of documents\"\n",
    "\n",
    "\n",
    "lcs = lcs_norm_word(O, S)\n",
    "\n",
    "\n",
    "###\n",
    "assert lcs==20/27., \"Incorrect LCS value, expected about 0.7408, got \"+str(lcs)\n",
    "\n",
    "print('Test passed!')\n",
    "print('LCS = ', lcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell runs a more rigorous test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "# test lcs implementation\n",
    "# params: complete_df from before, and lcs_norm_word function\n",
    "tests.test_lcs(df, lcs_norm_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, take a look at a few resultant values for `lcs_norm_word`. Just like before, you should see that higher values correspond to higher levels of plagiarism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test on your own\n",
    "test_df = df.copy()\n",
    "\n",
    "test_range = range(5) # look at first few rows, can change this range\n",
    "\n",
    "# iterate through first few docs and calculate LCS\n",
    "for test_index in test_range:\n",
    "    print('Category (0-3): ', test_df.loc[test_index, 'Category'])\n",
    "    \n",
    "    # calculate lcs\n",
    "    lcs_val = lcs_norm_word(test_df.loc[test_df.loc[test_index,'Orig_idx'], 'Text'], \n",
    "                            df.loc[test_index, 'Text'])\n",
    "    print(lcs_val)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Create features & determines models to test\n",
    "\n",
    "Now that you've completed the feature calculation functions, it's time to actually create multiple features and decide on which ones to use in your final model!\n",
    "\n",
    "In the below cells, you're given two helper functions to help you create ,multiple features and add the values as additional columns to an existing dataframe. These functions will be used to create the features you want to use in your final model; it will be up to you to decide on how many containment features to use and whether or not to use least common subsequence as a feature in your model.\n",
    "\n",
    "### Creating multiple containment features\n",
    "\n",
    "Your completed `calculate_containment` function will be called in the next cell, which defines the helper function `create_containment_features`. \n",
    "\n",
    "**This function adds containment features to an existing, passed in dataframe `df` based upon the passed in arguments.** This gives you the ability to easily create several containment features of different n-gram lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Do not need to change ##\n",
    "# Function creates containment features for all files in a df and adds them to the given dataframe\n",
    "def create_containment_features(df, n, column_name):\n",
    "    \n",
    "    # iterates through dataframe rows\n",
    "    for index in df.index:\n",
    "        # Computes features using function above for *answers* (not original files)\n",
    "        if df.loc[index,'Category'] > -1:\n",
    "            df.loc[index, column_name] = calculate_containment(df, n, index)\n",
    "        # Sets value to -1 for original tasks \n",
    "        else:\n",
    "            df.loc[index, column_name] = -1\n",
    "            \n",
    "    # returns nothing because dataframe is directly modifies\n",
    "    print(str(n) + '-gram containment features created!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating LCS features\n",
    "\n",
    "Below, your complete function is used to create LCS features and add them as columns to an existing dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Do not need to change ##\n",
    "# Function creates lcs feature and add it to the dataframe\n",
    "def create_lcs_features(df, column_name='lcs_word'):\n",
    "    \n",
    "    # iterate through files in dataframe\n",
    "    for index in df.index:\n",
    "        \n",
    "        # Computes LCS_norm words feature using function above for answer tasks\n",
    "        if df.loc[index,'Category'] > -1:\n",
    "            df.loc[index, column_name] = lcs_norm_word(df.loc[df.loc[index,'Orig_idx'], 'Text'],\n",
    "                                                       df.loc[index, 'Text'])\n",
    "        # Sets to -1 for original tasks \n",
    "        else:\n",
    "            df.loc[index, column_name] = -1\n",
    "\n",
    "    # nothing is returned, dataframe is directly modified\n",
    "    print('LCS features created!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Create features\n",
    "\n",
    "The paper suggests calculating the following features: containment *1-gram to 5-gram* and *longest common subsequence*. \n",
    "> In this exercise, we suggest that you increase the n-gram range from *1-gram to 7-gram* as well as calculate *longest common subsequence*. \n",
    "\n",
    "This will give you 8 different features to choose from; defining and comparing 8 different features allows you to discard any features that seem redundant or bad, and choose to use the best features for your final model!\n",
    "\n",
    "In the below cell **define an n-gram range**; these will be the n's you use to create n-gram containment features. The rest of the feature creation code is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select ngram range, ex. [1, 3, 5]\n",
    "ngram_range = None\n",
    "\n",
    "\n",
    "# following code may take a moment to run\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "features_list = []\n",
    "\n",
    "# Create features in a features_df\n",
    "features_df = df.copy()\n",
    "\n",
    "# Calculate features for containment for ngrams in range\n",
    "for n in ngram_range:\n",
    "    column_name = 'c_'+str(n)\n",
    "    features_list.append(column_name)\n",
    "    # create containment features\n",
    "    create_containment_features(features_df, n, column_name)\n",
    "\n",
    "# Calculate features for LCS_Norm Words \n",
    "features_list.append('lcs_word')\n",
    "create_lcs_features(features_df, 'lcs_word')\n",
    "\n",
    "# Check Dataframe\n",
    "print()\n",
    "print('Features: ', features_list)\n",
    "features_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "# Get *only* the features\n",
    "features_only = features_df[features_list].copy()\n",
    "\n",
    "features_only.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlated Features\n",
    "\n",
    "You should use feature correlation across the *entire* dataset to determine which features are ***too*** **highly correlated** with each other to include both features in a single model. For this analysis, you can use the *entire* dataset due to the small sample size we have. \n",
    "\n",
    "All of our features try to measure the similarity between two texts. Since our features are designed to measure similarity, it is expected that these features will be highly correlated. Generally, a Naive Bayes classifier relies on the assumption that features are *not* highly correlated; highly correlated features may ***over inflate the importance*** of a single feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "# Create correlation matrix for just Features to determine different models to test\n",
    "corr_matrix = features_only.corr().abs().round(2)\n",
    "\n",
    "# display shows all of a dataframe\n",
    "display(corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### EXERCISE: Selecting \"good\" features by correlation value\n",
    "\n",
    "Complete the `select_features` function below. This function should take in a dataframe that includes all our data and the extracted features, and a list of features that you want to include in your training/test datasets. You may assume that feature_list takes the form of a list of column names, ex. `['c_1', 'lcs_word']`.\n",
    "\n",
    "* You will have to decide on a **cutoff** value for features that are *too* highly correlated.\n",
    "* If you cannot find features that are less correlated than some cutoff value, it is suggested that you increase the number of features (longer n-grams) to choose from or simply use *only one or two* features in your final model to avoid highly correlated features.\n",
    "* Your function should return two tuples:\n",
    "    * `(train_x, train_y)`, a dataframe of training features and a list of corresponding class labels (0/1)\n",
    "    * `(test_x, test_y)`, a dataframe of test features and a list of corresponding class labels (0/1)\n",
    "\n",
    "Recall that our complete dataframe has a Datatype column that indicates whether data should be `train` or `test` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_features(features_df, feature_list):\n",
    "    '''Creates training and test data based on selected features.\n",
    "       :param features_df: dataframe with all fies, datatypes, and features\n",
    "       :param feature_list: a list of feature names to select, ex ['c_1', 'lcs_word']\n",
    "       :return: two tuples (train_x, train_y) and (text_x, test_y) of features and class labels.'''\n",
    "    \n",
    "    train_x = None\n",
    "    train_y = None\n",
    "    \n",
    "    test_x = None\n",
    "    test_y = None\n",
    "    \n",
    "    return (train_x, train_y), (test_x, test_y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "# test that feature selection returns the correct datatypes\n",
    "# params: features_df from a few cells above, and select_features function\n",
    "tests.test_selection(features_df, select_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing your function code, it's time to define a list of features that you want to include in your final model. Write a feature_list, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select your best, low-correlated features\n",
    "# feature_list should be a list of feature names, ex: ['c_1`, 'c_2', 'lcs_word']\n",
    "feature_list = ['c_1', 'c_4', 'lcs_word']\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "(train_x, train_y), (test_x, test_y) = select_features(features_df, feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Question 3: How did you decide on which features to include in your final model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "(Write here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Modeling\n",
    "\n",
    "You've decided on which features to use and gotten your training and test data in order. Finally, it's time to define and evaluate a model!\n",
    "\n",
    "We suggest you use either a logistic or a naive bayes classifier to complete this binary, plagiarism classification task. There are some options to explore, and it will be up to you to test out a variety of models and choose the best one.\n",
    " \n",
    "\n",
    "### EXERCISE: Train and evaluate a model\n",
    "\n",
    "For this exercise, complete the following function `model_acc`. This should take in a few parameters:\n",
    "* **model**: the model to test, such as a MultinomialNB(), etc.\n",
    "* **train_x, train_y**: the training features and class labels (0/1), respectively\n",
    "* **test_x, test_y**: the test features and class labels (0/1), respectively\n",
    "\n",
    "### Metrics\n",
    "\n",
    "Your function should train the model on the training data and evaluate it on the test data. \n",
    "\n",
    "In this case, we care about accuracy as well as the number of false positives and false negatives. We *do not* want to miss any plagiarized cases and we don't want to have any false positives either! \n",
    "\n",
    "In this vein, you function should return both the accuracy and a confusion matrix.\n",
    "\n",
    "> Your model will ultimately be evaluated on its test accuracy; you should be able to consistently get over **90%** accuracy on the test data. \n",
    "\n",
    "You'll also be asked to justify your choice of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Naive Bayes Classifier (using gaussian because features are continuous values)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "def model_metrics(model, train_x, train_y, test_x, test_y):\n",
    "    '''Trains a model and returns the accuracy and confusion matrix.\n",
    "       :param model: model to train, ex. MultinomialNB()\n",
    "       :param train_x, train_y: training features and class labels\n",
    "       :param test_x, test_y: test features and class labels'''\n",
    "\n",
    "    # Train the model with the training data\n",
    "    # Create predictions on test data and calculate desired metrics\n",
    "    \n",
    "    accuracy = None\n",
    "    confusion_matrix = None\n",
    "    \n",
    "    return accuracy, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "# test that feature selection returns the correct datatypes\n",
    "# params: features_df from a few cells above, and select_features function\n",
    "tests.test_model_metrics(model_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compare at least two models\n",
    "# print out stats/results\n",
    "\n",
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Question 4: How did you decide on the type of model to use? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer**:\n",
    "\n",
    "(Write here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Further Directions\n",
    "\n",
    "There are many ways to improve or add on to this project to expand your learning or make this more of a unique project for you. A few ideas are listed below:\n",
    "* Train a classifier to predict the *category* of plagiarism and not just plagiarized or not.\n",
    "* Utilize a different and larger dataset to see if this model can be extended to non-computer science, submitted answers.\n",
    "* Use language or character-level analysis to find different and more features.\n",
    "* Write a complete pipeline function that accepts a source text and submitted text file, and classifies the submitted text as plagiarized or not.\n",
    "\n",
    "These are all just options for extending your work. If you've completed all the tasks in this notebook and run your code, you've completed a real-world application, and can proceed to submit your work. Great job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
